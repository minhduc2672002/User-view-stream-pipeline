{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "util_dir: /home/tranminhduc/dec_project/de-coaching-lab-main/spark/12-project/util\n"
     ]
    }
   ],
   "source": [
    "# import pyspark.sql.functions as f\n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.functions import col,from_json\n",
    "# from pyspark.sql.types import StringType, StructType, StructField, LongType, ArrayType, MapType,IntegerType,TimestampType\n",
    "# from pyspark.sql.functions import *\n",
    "\n",
    "from util.config import Config\n",
    "from util.logger import Log4j\n",
    "\n",
    "# import psycopg2\n",
    "\n",
    "conf = Config()\n",
    "spark_conf = conf.spark_conf\n",
    "kaka_conf = conf.kafka_conf\n",
    "postgres_conf =conf.postgres_conf\n",
    "\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .config(conf=spark_conf) \\\n",
    "#     .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,org.postgresql:postgresql:42.7.3\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# log = Log4j(spark)\n",
    "# log.info(f\"spark_conf: {spark_conf.getAll()}\")\n",
    "# log.info(f\"kafka_conf: {kaka_conf.items()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbname=postgres_conf['dbname'],\n",
    "user=postgres_conf['user'],\n",
    "password=postgres_conf['password'],\n",
    "host=postgres_conf['host'],\n",
    "port=postgres_conf['port']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jdbc:postgresql://postgres:5432/behavior\n"
     ]
    }
   ],
   "source": [
    "print(f\"jdbc:postgresql://{postgres_conf['host']}:{postgres_conf['port']}/{postgres_conf['dbname']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'behavior'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbname[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_postgres():\n",
    "    try:\n",
    "        connection = psycopg2.connect(\n",
    "            dbname=postgres_conf['dbname'],\n",
    "            user=postgres_conf['user'],\n",
    "            password=postgres_conf['password'],\n",
    "            host=postgres_conf['host'],\n",
    "            port=postgres_conf['port']\n",
    "        )\n",
    "        print(\"Connection to PostgreSQL established.\")\n",
    "        return connection\n",
    "    except Exception as e:\n",
    "        print(\"Error connecting to PostgreSQL:\", e)\n",
    "        return None\n",
    "        \n",
    "def create_table(connection,create_table_query):\n",
    "    try:\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(create_table_query)\n",
    "        connection.commit()\n",
    "        print(\"Table created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error creating table:\", e)\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        \n",
    "def save_to_postgres(df,dbtable):\n",
    "    df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres:5432/behavior\") \\\n",
    "    .option(\"dbtable\", f\"{dbtable}\") \\\n",
    "    .option(\"user\", \"admin\") \\\n",
    "    .option(\"password\", \"admin\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_to_table(partition,insert_query,columns):\n",
    "    try:\n",
    "        #Connect to Postgres\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=postgres_conf['dbname'],\n",
    "            user=postgres_conf['user'],\n",
    "            password=postgres_conf['password'],\n",
    "            host=postgres_conf['host'],\n",
    "            port=postgres_conf['port']\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        if partition:\n",
    "            # Duyệt qua từng hàng trong phân vùng và lưu vào PostgreSQL\n",
    "            for row in partition:\n",
    "                try:\n",
    "                    values = [getattr(row, col) for col in columns]\n",
    "                    cursor.execute(insert_query,values)\n",
    "                except Exception as e:\n",
    "                    # Log lỗi chi tiết khi có lỗi trong từng dòng\n",
    "                    print(f\"Lỗi khi chèn dòng {row}: {e}\")\n",
    "        \n",
    "        # Ghi thay đổi và đóng kết nối\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        # Log lỗi khi có lỗi kết nối hoặc lỗi chung trong phân vùng\n",
    "        print(f\"Lỗi trong phân vùng: {e}\")\n",
    "    finally:\n",
    "        # Đảm bảo kết nối được đóng lại\n",
    "        if 'conn' in locals() and conn:\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "\n",
    "def insert_to_table(partition, insert_query, columns):\n",
    "    # Kết nối đến PostgreSQL\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=postgres_conf['dbname'],\n",
    "        user=postgres_conf['user'],\n",
    "        password=postgres_conf['password'],\n",
    "        host=postgres_conf['host'],\n",
    "        port=postgres_conf['port']\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    if partition:\n",
    "        # Duyệt qua từng hàng trong phân vùng và lưu vào PostgreSQL\n",
    "        for row in partition:\n",
    "            values = [getattr(row, col) for col in columns]\n",
    "            cursor.execute(insert_query, values)\n",
    "\n",
    "    # Ghi thay đổi sau khi chèn thành công\n",
    "    conn.commit()\n",
    "\n",
    "    # Đảm bảo đóng kết nối và cursor\n",
    "    cursor.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_to_dim_browser(df_browser):\n",
    "    columns = [\"browser_key\",\"browser_name\"]\n",
    "    column_names = \",\".join(columns)\n",
    "    placeholders = ', '.join(['%s'] * len(columns))\n",
    "    insert_query = f\"INSERT INTO dim_browser ({column_names}) VALUES ({placeholders}) ON CONFLICT (browser_key) DO NOTHING\"\n",
    "    df_browser.foreachPartition(lambda partition: insert_to_table(partition,insert_query,columns))\n",
    "\n",
    "def insert_to_dim_os(df_os):\n",
    "    columns = [\"os_key\",\"os_name\"]\n",
    "    column_names = \",\".join(columns)\n",
    "    placeholders = ', '.join(['%s'] * len(columns))\n",
    "    insert_query = f\"INSERT INTO dim_operating_system ({column_names}) VALUES ({placeholders}) ON CONFLICT (os_key) DO NOTHING\"\n",
    "    df_os.foreachPartition(lambda partition: insert_to_table(partition,insert_query,columns))\n",
    "\n",
    "def insert_to_dim_reference(df_reference):\n",
    "    columns = [\"reference_key\",\"reference_domain\",\"is_self_reference\"]\n",
    "    column_names = \",\".join(columns)\n",
    "    placeholders = ', '.join(['%s'] * len(columns))\n",
    "    insert_query = f\"INSERT INTO dim_reference_domain ({column_names}) VALUES ({placeholders}) ON CONFLICT (reference_key) DO NOTHING\"\n",
    "    df_reference.foreachPartition(lambda partition: insert_to_table(partition,insert_query,columns))\n",
    "\n",
    "\n",
    "def insert_to_fact_view(df_fact_view):\n",
    "    columns = [\"key\",\"date_key\",\"location_key\",\"product_key\",\"store_id\",\"reference_key\",\"browser_key\",\"os_key\",\"total_view\"]\n",
    "    column_names = \",\".join(columns)\n",
    "    placeholders = ', '.join(['%s'] * len(columns))\n",
    "    insert_query = f\"\"\"INSERT INTO fact_view ({column_names}) \n",
    "                        VALUES ({placeholders}) \n",
    "                        ON CONFLICT (key) \n",
    "                        DO UPDATE SET total_view = fact_view.total_view + EXCLUDED.total_view;\"\"\"\n",
    "    df_fact_view.foreachPartition(lambda partition: insert_to_table(partition,insert_query,columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to PostgreSQL established.\n",
      "Table created successfully.\n"
     ]
    }
   ],
   "source": [
    "connection = connect_to_postgres()\n",
    "create_table_query = \"\"\"\n",
    "DROP TABLE IF EXISTS Dim_Date;\n",
    "CREATE TABLE  Dim_Date (\n",
    "    datetime_key INT PRIMARY KEY,\n",
    "    full_date DATE,\n",
    "    day_of_week VARCHAR(10),\n",
    "    day_of_week_short VARCHAR(10),\n",
    "    hour INT,\n",
    "    day_of_month INT,\n",
    "    month INT,\n",
    "    year INT\n",
    ");\n",
    "\n",
    "DROP TABLE IF EXISTS Dim_Product;\n",
    "CREATE TABLE  Dim_Product (\n",
    "    product_key INT PRIMARY KEY,\n",
    "    product_name VARCHAR(50)\n",
    ");\n",
    "\n",
    "DROP TABLE IF EXISTS Dim_Location;\n",
    "CREATE TABLE  Dim_Location (\n",
    "    location_key INT PRIMARY KEY,\n",
    "    country_iso2 VARCHAR(20),\n",
    "    country_iso3 VARCHAR(20) ,\n",
    "    country_name VARCHAR(50),\n",
    "    country_nicename VARCHAR(50),\n",
    "    country_numcode INT,\n",
    "    country_phonecode INT\n",
    ");\n",
    "\n",
    "DROP TABLE IF EXISTS Dim_Reference_Domain;\n",
    "CREATE TABLE  Dim_Reference_Domain (\n",
    "    reference_key INT PRIMARY KEY,\n",
    "    reference_domain VARCHAR(255),\n",
    "    is_self_reference BOOLEAN\n",
    ");\n",
    "\n",
    "DROP TABLE IF EXISTS Dim_Operating_System;\n",
    "CREATE TABLE  Dim_Operating_System (\n",
    "    os_key INT PRIMARY KEY,\n",
    "    os_name VARCHAR(50)\n",
    ");\n",
    "\n",
    "DROP TABLE IF EXISTS Dim_Browser;\n",
    "CREATE TABLE  Dim_Browser (\n",
    "    browser_key INT PRIMARY KEY,\n",
    "    browser_name VARCHAR(50)\n",
    ");\n",
    "\n",
    "DROP TABLE IF EXISTS Fact_View;\n",
    "CREATE TABLE  Fact_View (\n",
    "    key VARCHAR(255) PRIMARY KEY, \n",
    "    product_key INT NOT NULL,\n",
    "    location_key INT NOT NULL,\n",
    "    date_key INT NOT NULL,\n",
    "    reference_key INT NOT NULL,\n",
    "    os_key INT NOT NULL,\n",
    "    browser_key INT NOT NULL,\n",
    "    store_id INT,\n",
    "    total_view INT\n",
    ");\n",
    "\"\"\"\n",
    "create_table(connection,create_table_query)\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_behavior = spark.read.format('parquet').load('/data/data_behavior/product_view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def get_partition_size(index, iterator):\n",
    "    partition_size = sum(sys.getsizeof(row) for row in iterator)\n",
    "    return [(index, partition_size)]\n",
    "    \n",
    "partition_sizes = df_behavior.rdd.mapPartitionsWithIndex(get_partition_size).collect()\n",
    "for index, size in partition_sizes:\n",
    "    print(f\"Partition {index}: {size / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE DIM DATE\n",
    "\n",
    "def create_dim_date(spark):\n",
    "    from pyspark.sql.functions import expr,dayofweek,date_format,dayofmonth,month,year,hour\n",
    "    \n",
    "    # Tạo DataFrame có dãy ngày từ 01-10-2024 đến 10-10-2024\n",
    "    df_dates = spark.range(1).select(expr(\"sequence(to_timestamp('2024-01-01 00:00:00'), to_timestamp('2025-01-01 00:00:00'), interval 1 hour) as timestamp_seq\"))\n",
    "    \n",
    "    # Explode chuỗi ngày để có mỗi ngày một dòng\n",
    "    df_dates_generate = df_dates.selectExpr(\"explode(timestamp_seq) as timestamp\")\n",
    "    # Tao cac cot lien quan\n",
    "    df_dates_generate_column = df_dates_generate \\\n",
    "    .withColumn('datetime_key',date_format(\"timestamp\", \"yyyyMMddHH\").cast('int')) \\\n",
    "    .withColumn('full_date',col('timestamp').cast('date')) \\\n",
    "    .withColumn(\"day_of_week\", date_format(\"full_date\", \"EEEE\")) \\\n",
    "    .withColumn('day_of_week_short',dayofweek(\"full_date\")) \\\n",
    "    .withColumn('day_of_month',dayofmonth(\"full_date\")) \\\n",
    "    .withColumn('month',month(\"full_date\")) \\\n",
    "    .withColumn('year',year(\"full_date\")) \\\n",
    "    .withColumn('hour',hour(\"timestamp\")) \\\n",
    "    .drop(\"timestamp\")\n",
    "    \n",
    "    save_to_postgres(df_dates_generate_column,'dim_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE DIM LOCATION\n",
    "\n",
    "def create_dim_location(spark):\n",
    "    from pyspark.sql.functions import hash,abs,col,when\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"iso\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"nicename\", StringType(), True),\n",
    "        StructField(\"iso3\", StringType(), True),\n",
    "        StructField(\"numcode\", IntegerType(), True),\n",
    "        StructField(\"phonecode\", IntegerType(), True)\n",
    "    ])\n",
    "    df_location = spark.read \\\n",
    "    .format('csv') \\\n",
    "    .option('header',True) \\\n",
    "    .schema(schema) \\\n",
    "    .option('path','/data/country.csv') \\\n",
    "    .load()\n",
    "\n",
    "    undefined_row = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "            0 AS id\n",
    "            , 'Undefined' AS iso\n",
    "            , 'Undefined' AS name\n",
    "            , 'Undefined' AS nicename\n",
    "            , 'Undefined' AS iso3\n",
    "            , -1 AS numcode\n",
    "            , -1 AS phonecode\n",
    "    \"\"\")\n",
    "    df_location_final = df_country \\\n",
    "    .union(undefined_row) \\\n",
    "    .orderBy('id') \\\n",
    "    .withColumn('location_key',abs(hash('iso'))) \\\n",
    "    .selectExpr(\"location_key\",\n",
    "               \"iso AS country_iso2\",\n",
    "               \"iso3 AS country_iso3\",\n",
    "               \"name AS country_name\",\n",
    "               \"nicename AS country_nicename\",\n",
    "               \"numcode AS country_numcode\",\n",
    "               \"phonecode AS country_phonecode\")\n",
    "\n",
    "    save_to_postgres(df_location_final,'dim_location')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE DIM PRODUCT\n",
    "def create_dim_product(spark):\n",
    "    from pyspark.sql.functions import hash,abs,col,when\n",
    "    \n",
    "    schema = StructType([\n",
    "            StructField(\"id\", IntegerType(), True),\n",
    "            StructField(\"name\", StringType(), True),\n",
    "        ])\n",
    "    \n",
    "    df_product = spark.read \\\n",
    "    .format('csv') \\\n",
    "    .option('header',True) \\\n",
    "    .schema(schema) \\\n",
    "    .option('path','/data/dim_product.csv') \\\n",
    "    .load()\n",
    "\n",
    "    df_product_final = df_product.selectExpr(\"id AS product_key\",\"name AS product_name\")\n",
    "    save_to_postgres(df_product_final,\"dim_product\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split, regexp_extract,size,upper\n",
    "from pyspark.sql.functions import hash,abs,col,when,concat,sha2,md5,coalesce,lit\n",
    "\n",
    "extract_current_domain = split(col(\"current_url\"),\"/\")[2]\n",
    "extract_reference_domain = split(col(\"referrer_url\"),\"/\")[2]\n",
    "num_parts = size(split(extract_current_domain, r\"\\.\"))\n",
    "\n",
    "\n",
    "\n",
    "df_behavior_extract_domain  = df_behavior \\\n",
    ".withColumn('current_domain',extract_current_domain) \\\n",
    ".withColumn('reference_domain',extract_reference_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HANDLE FACT VIEW\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# GENKEY LOCATION\n",
    "extract_coutnry_code = upper(split(extract_current_domain,r\"\\.\")[num_parts -1])\n",
    "fix_country_code =  expr(\"\"\"CASE\n",
    "                                WHEN country_code = 'COM' THEN 'US'\n",
    "                                WHEN country_code = 'AFRICA'  THEN 'BF'\n",
    "                                WHEN country_code = 'MEDIA' THEN 'LY'\n",
    "                                WHEN country_code = 'STORE' THEN 'CU'\n",
    "                                WHEN country_code = '' THEN 'Undefined'\n",
    "                                ELSE country_code\n",
    "                            END AS country_code\n",
    "                        \"\"\")\n",
    "gen_location_key = abs(hash('country_code'))\n",
    "\n",
    "#GENKEY DATE\n",
    "gen_date_key = date_format(\"local_time\", \"yyyyMMddHH\").cast('int')\n",
    "\n",
    "#GENKEY REFERENCE\n",
    "handle_refernce_null = expr(\"IFNULL(reference_domain,'Undefined') AS reference_domain\")\n",
    "is_self_reference = expr(\"\"\" CASE \n",
    "                                WHEN current_domain = reference_domain THEN True\n",
    "                                ELSE False\n",
    "                            END AS is_self_reference\n",
    "                        \"\"\")\n",
    "gen_reference_key = abs(hash('reference_domain'))\n",
    "\n",
    "#GENKEY BROWER\n",
    "def parse_browser(ua):\n",
    "    user_agent = parse(ua)\n",
    "    return user_agent.browser.family\n",
    "parse_browser_udf = udf(parse_browser, returnType=StringType())\n",
    "gen_browser_key = abs(hash('browser'))\n",
    "\n",
    "#GENKEY OS\n",
    "def parse_os(ua):\n",
    "    user_agent = parse(ua)\n",
    "    return user_agent.os.family\n",
    "    \n",
    "parse_os_udf = udf(parse_os, returnType=StringType())\n",
    "gen_os_key = abs(hash('os'))\n",
    "\n",
    "#HANDLE PRODUCT\n",
    "handle_null_product_id = expr(\"IFNULL(product_id,-1)\")\n",
    "\n",
    "\n",
    "df_behavior_genkey = df_behavior_extract_domain.limit(10) \\\n",
    ".withColumn('country_code',extract_coutnry_code) \\\n",
    ".withColumn('country_code',fix_country_code) \\\n",
    ".withColumn('location_key',gen_location_key) \\\n",
    ".withColumn('date_key',gen_date_key) \\\n",
    ".withColumn('reference_domain',handle_refernce_null) \\\n",
    ".withColumn('is_self_reference',is_self_reference) \\\n",
    ".withColumn('reference_key',gen_reference_key) \\\n",
    ".withColumn('browser',parse_browser_udf(\"user_agent\")) \\\n",
    ".withColumn('browser_key',gen_browser_key) \\\n",
    ".withColumn('os',parse_os_udf(\"user_agent\")) \\\n",
    ".withColumn('os_key',gen_os_key) \\\n",
    ".withColumn(\"product_id\",handle_null_product_id) \\\n",
    ".withColumnRenamed(\"product_id\",\"product_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gen_fact_key = md5(\n",
    "        concat(\n",
    "            coalesce(col(\"date_key\").cast(\"string\"), lit(\"\")),\n",
    "            coalesce(col(\"location_key\").cast(\"string\"), lit(\"\")),\n",
    "            coalesce(col(\"product_key\").cast(\"string\"), lit(\"\")),\n",
    "            coalesce(col(\"store_id\").cast(\"string\"), lit(\"\")),\n",
    "            coalesce(col(\"reference_key\").cast(\"string\"), lit(\"\")),\n",
    "            coalesce(col(\"browser_key\").cast(\"string\"), lit(\"\")),\n",
    "            coalesce(col(\"os_key\").cast(\"string\"), lit(\"\"))\n",
    "        )\n",
    "    )\n",
    "\n",
    "fact_view = df_behavior_genkey \\\n",
    ".groupBy(\"date_key\",\n",
    "        \"location_key\",\n",
    "        \"product_key\",\n",
    "        \"store_id\",\n",
    "       \"reference_key\",\n",
    "       \"browser_key\",\n",
    "       \"os_key\"\n",
    "       ) \\\n",
    ".agg(expr(\"count(*) AS total_view\")) \\\n",
    ".withColumn(\"key\",gen_fact_key)\n",
    "insert_to_fact_view(fact_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STORE DIM BROWSER\n",
    "df_dim_browser = df_behavior_genkey \\\n",
    ".selectExpr(\"browser_key\",\n",
    "            \"browser AS browser_name\") \\\n",
    ".distinct()\n",
    "insert_to_dim_browser(df_dim_browser)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STORE DIM OS\n",
    "df_dim_os = df_behavior_genkey \\\n",
    ".selectExpr(\"os_key\",\n",
    "            \"os AS os_name\") \\\n",
    ".distinct()\n",
    "insert_to_dim_os(df_dim_os)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STORE DIM REFER\n",
    "df_dim_refer = df_behavior_genkey \\\n",
    ".selectExpr(\"reference_key\",\n",
    "            \"reference_domain\",\n",
    "            \"is_self_reference\") \\\n",
    ".distinct()\n",
    "insert_to_dim_reference(df_dim_refer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def process_batch(df: DataFrame, batch_id: int):\n",
    "    df_behavior = df\n",
    "    extract_current_domain = split(col(\"current_url\"),\"/\")[2]\n",
    "    extract_reference_domain = split(col(\"referrer_url\"),\"/\")[2]\n",
    "    num_parts = size(split(extract_current_domain, r\"\\.\"))\n",
    "    \n",
    "    \n",
    "    \n",
    "    df_behavior_extract_domain  = df_behavior \\\n",
    "    .withColumn('current_domain',extract_current_domain) \\\n",
    "    .withColumn('reference_domain',extract_reference_domain)\n",
    "\n",
    "    extract_coutnry_code = upper(split(extract_current_domain,r\"\\.\")[num_parts -1])\n",
    "    fix_country_code =  expr(\"\"\"CASE\n",
    "                                    WHEN country_code = 'COM' THEN 'US'\n",
    "                                    WHEN country_code = 'AFRICA'  THEN 'BF'\n",
    "                                    WHEN country_code = 'MEDIA' THEN 'LY'\n",
    "                                    WHEN country_code = 'STORE' THEN 'CU'\n",
    "                                    WHEN country_code = '' THEN 'Undefined'\n",
    "                                    ELSE country_code\n",
    "                                END AS country_code\n",
    "                            \"\"\")\n",
    "    gen_location_key = abs(hash('country_code'))\n",
    "    \n",
    "    #GENKEY DATE\n",
    "    gen_date_key = date_format(\"local_time\", \"yyyyMMddHH\").cast('int')\n",
    "    \n",
    "    #GENKEY REFERENCE\n",
    "    handle_refernce_null = expr(\"IFNULL(reference_domain,'Undefined') AS reference_domain\")\n",
    "    is_self_reference = expr(\"\"\" CASE \n",
    "                                    WHEN current_domain = reference_domain THEN True\n",
    "                                    ELSE False\n",
    "                                END AS is_self_reference\n",
    "                            \"\"\")\n",
    "    gen_reference_key = abs(hash('reference_domain'))\n",
    "    \n",
    "    #GENKEY BROWER\n",
    "    parse_browser_udf = udf(lambda ua: parse(ua).browser.family, StringType())\n",
    "    gen_browser_key = abs(hash('browser'))\n",
    "    \n",
    "    #GENKEY OS\n",
    "    parse_os_udf = udf(lambda ua: parse(ua).os.family, StringType())\n",
    "    gen_os_key = abs(hash('os'))\n",
    "    \n",
    "    #HANDLE PRODUCT\n",
    "    handle_null_product_id = expr(\"IFNULL(product_id,-1)\")\n",
    "    \n",
    "    \n",
    "    df_behavior_genkey = df_behavior_extract_domain \\\n",
    "    .withColumn('country_code',extract_coutnry_code) \\\n",
    "    .withColumn('country_code',fix_country_code) \\\n",
    "    .withColumn('location_key',gen_location_key) \\\n",
    "    .withColumn('date_key',gen_date_key) \\\n",
    "    .withColumn('reference_domain',handle_refernce_null) \\\n",
    "    .withColumn('is_self_reference',is_self_reference) \\\n",
    "    .withColumn('reference_key',gen_reference_key) \\\n",
    "    .withColumn('browser',parse_browser_udf(\"user_agent\")) \\\n",
    "    .withColumn('browser_key',gen_browser_key) \\\n",
    "    .withColumn('os',parse_os_udf(\"user_agent\")) \\\n",
    "    .withColumn('os_key',gen_os_key) \\\n",
    "    .withColumn(\"product_id\",handle_null_product_id) \\\n",
    "    .withColumnRenamed(\"product_id\",\"product_key\")\n",
    "\n",
    "    gen_fact_key = md5(\n",
    "            concat(\n",
    "                coalesce(col(\"date_key\").cast(\"string\"), lit(\"\")),\n",
    "                coalesce(col(\"location_key\").cast(\"string\"), lit(\"\")),\n",
    "                coalesce(col(\"product_key\").cast(\"string\"), lit(\"\")),\n",
    "                coalesce(col(\"store_id\").cast(\"string\"), lit(\"\")),\n",
    "                coalesce(col(\"reference_key\").cast(\"string\"), lit(\"\")),\n",
    "                coalesce(col(\"browser_key\").cast(\"string\"), lit(\"\")),\n",
    "                coalesce(col(\"os_key\").cast(\"string\"), lit(\"\"))\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    fact_view = df_behavior_genkey \\\n",
    "    .groupBy(\"date_key\",\n",
    "            \"location_key\",\n",
    "            \"product_key\",\n",
    "            \"store_id\",\n",
    "           \"reference_key\",\n",
    "           \"browser_key\",\n",
    "           \"os_key\"\n",
    "           ) \\\n",
    "    .agg(expr(\"count(*) AS total_view\")) \\\n",
    "    .withColumn(\"key\",gen_fact_key)\n",
    "    insert_to_fact_view(fact_view)\n",
    "\n",
    "    ## STORE DIM BROWSER\n",
    "    df_dim_browser = df_behavior_genkey \\\n",
    "    .selectExpr(\"browser_key\",\n",
    "                \"browser AS browser_name\") \\\n",
    "    .distinct()\n",
    "    insert_to_dim_browser(df_dim_browser)\n",
    "    \n",
    "    ## STORE DIM OS\n",
    "    df_dim_os = df_behavior_genkey \\\n",
    "    .selectExpr(\"os_key\",\n",
    "                \"os AS os_name\") \\\n",
    "    .distinct()\n",
    "    insert_to_dim_os(df_dim_os)\n",
    "\n",
    "    ## STORE DIM REFER\n",
    "    df_dim_refer = df_behavior_genkey \\\n",
    "    .selectExpr(\"reference_key\",\n",
    "                \"reference_domain\",\n",
    "                \"is_self_reference\") \\\n",
    "    .distinct()\n",
    "    insert_to_dim_reference(df_dim_refer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/01 08:16:30 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/11/01 08:23:02 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    }
   ],
   "source": [
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"auto.offset.reset\", \"earliest\") \\\n",
    "    .option(\"startingOffsets\",\"earliest\") \\\n",
    "    .options(**kaka_conf) \\\n",
    "    .load()\n",
    "\n",
    "# maxOffsetsPerTrigger\n",
    "def normalized_df(df):\n",
    "    schema = StructType([\n",
    "        StructField(\"_id\", StringType(), True),\n",
    "        StructField(\"time_stamp\", StringType(), True),\n",
    "        StructField(\"ip\", StringType(), True),\n",
    "        StructField(\"user_agent\", StringType(), True),\n",
    "        StructField(\"resolution\", StringType(), True),\n",
    "        StructField(\"device_id\", StringType(), True),\n",
    "        StructField(\"api_version\", StringType(), True),\n",
    "        StructField(\"store_id\", StringType(), True),\n",
    "        StructField(\"local_time\", TimestampType(), True),\n",
    "        StructField(\"show_recommendation\", StringType(), True),\n",
    "        StructField(\"current_url\", StringType(), True),\n",
    "        StructField(\"referrer_url\", StringType(), True),\n",
    "        StructField(\"email_address\", StringType(), True),\n",
    "        StructField(\"collection\", StringType(), True),\n",
    "        StructField(\"product_id\", StringType(), True),\n",
    "        StructField(\"option\", ArrayType(StructType([\n",
    "            StructField(\"option_label\", StringType(), True),\n",
    "            StructField(\"option_id\", StringType(), True),\n",
    "            StructField(\"value_label\", StringType(), True),\n",
    "            StructField(\"value_id\", StringType(), True),\n",
    "        ])), True),\n",
    "        StructField(\"id\", StringType(), True)\n",
    "    ])\n",
    "    parsed_df = (\n",
    "        df.withColumn(\"json_data\", from_json(col(\"value\").cast(\"string\"), schema))\n",
    "        .select(\"json_data.*\")\n",
    "        .withColumn(\"time_stamp\", col(\"time_stamp\").cast(LongType()))\n",
    "        .withColumn(\"product_id\", col(\"product_id\").cast(IntegerType()))\n",
    "        .withColumn(\"store_id\", col(\"store_id\").cast(IntegerType()))\n",
    "        .withColumn(\"local_date\",f.to_date(col(\"local_time\"),\"yyyy-MM-dd\"))\n",
    "    )\n",
    "    return parsed_df\n",
    "\n",
    "query = df.transform(lambda df: normalized_df(df)) \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .option(\"checkpointLocation\", \"/data/data_behavior/kafka_checkpoint/\") \\\n",
    "    .start() \\\n",
    "    .awaitTermination()\n",
    "    # .option(\"maxOffsetsPerTrigger\",\"1000\") \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = df.transform(lambda df: normalized_df(df,schema)) \\\n",
    "#     .writeStream \\\n",
    "#     .format(\"console\") \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .trigger(processingTime='10 seconds') \\\n",
    "#     .option(\"checkpointLocation\", \"/tmp/kafka_checkpoint\") \\\n",
    "#     .start()\n",
    "\n",
    "\n",
    "# query = df.transform(lambda df: normalized_df(df,schema)) \\\n",
    "#     .writeStream \\\n",
    "#     .format(\"parquet\") \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .option(\"checkpointLocation\", \"/data/data_behavior/kafka_checkpoint/\") \\\n",
    "#     .option(\"path\", \"/data/data_behavior/product_view\") \\\n",
    "#     .start() \\\n",
    "#     .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HANDLE Dim Referer\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df_referer = df_behavior_extract_domain \\\n",
    ".selectExpr(\"IFNULL(reference_domain,'Undefined') AS reference_domain\",\n",
    "            \"\"\"\n",
    "            CASE \n",
    "                WHEN current_domain = reference_domain THEN True\n",
    "                ELSE False\n",
    "            END AS is_self_reference\n",
    "            \"\"\") \\\n",
    ".distinct() \\\n",
    ".withColumn('reference_key',abs(hash('reference_domain')))\n",
    "insert_to_dim_reference(df_referer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HANDLE DIM BROWER\n",
    "from pyspark.sql.functions import hash,abs,udf, col\n",
    "\n",
    "def parse_browser(ua):\n",
    "    user_agent = parse(ua)\n",
    "    return user_agent.browser.family\n",
    "parse_browser_udf = udf(parse_browser, returnType=StringType())\n",
    "\n",
    "df_browser = df_behavior \\\n",
    ".select(\"user_agent\") \\\n",
    "df_browser_repartition = df_browser.repartition(2)\n",
    "\n",
    "df_browser_final1 = df_browser_repartition \\\n",
    ".withColumn(\"browser\", parse_browser_udf(\"user_agent\")) \\\n",
    ".withColumn(\"browser_key\", abs(hash('browser'))) \\\n",
    ".selectExpr(\"browser_key\",\n",
    "            \"IF(browser = '','Undefined',browser) AS browser_name\") \\\n",
    ".distinct()\n",
    "\n",
    "insert_to_dim_browser(df_browser_final1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handle Dim OS\n",
    "from pyspark.sql.functions import hash,abs,udf, col\n",
    "\n",
    "def parse_os(ua):\n",
    "    user_agent = parse(ua)\n",
    "    return user_agent.os.family\n",
    "    \n",
    "parse_os_udf = udf(parse_os, returnType=StringType())\n",
    "\n",
    "\n",
    "\n",
    "df_os = df_behavior \\\n",
    ".select(\"user_agent\") \\\n",
    ".limit(100) \\\n",
    ".withColumn(\"os\", parse_os_udf(\"user_agent\")) \\\n",
    ".withColumn(\"os_key\", abs(hash('os'))) \\\n",
    ".selectExpr(\"os_key\",\n",
    "            \"IF(os = '','Undefined',os) AS os_name\") \\\n",
    ".distinct()\n",
    "insert_to_dim_os(df_os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_referer.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_to_dim_browser(df_browser_final1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_browser.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres:5432/behavior\") \\\n",
    "    .option(\"dbtable\", \"dim_browser\") \\\n",
    "    .option(\"user\", \"admin\") \\\n",
    "    .option(\"password\", \"admin\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_os.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres:5432/behavior\") \\\n",
    "    .option(\"dbtable\", \"dim_operating_system\") \\\n",
    "    .option(\"user\", \"admin\") \\\n",
    "    .option(\"password\", \"admin\") \\\n",
    "    .mode(\"ignore\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
